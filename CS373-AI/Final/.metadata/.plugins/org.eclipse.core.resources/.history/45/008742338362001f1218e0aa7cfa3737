/* Nathaniel Lim
 * CS373 - Final Project
 * May 18, 2010
 * Implicit Imitation 
 */

package project;
import java.util.Random;


public class Piece {	
	
	protected boolean isObserver;
	protected Piece mentor;
	protected ChessBoard world;
	private Coordinate pos;
	private int [][] tally;
	private int [][][] actionTally;
	private double [][] qMap;
	private double [] vMap;
	private double [] visibleQMap;
	private final double GAMMA = 0.85;
	private final double ALPHA = 0.6;
	private double EPSILON = 0.2;
	private int MENTOR_ACTION_INDEX = -1;
	Random r = new Random();
	
	protected PieceAction[] actions;
	
	public Coordinate getPosition(){
		return pos;
	}
	
	public void setPosition(Coordinate p){
		this.pos = p;
	}
	
	public void setMentor(Piece m){
		if (isObserver){
			this.mentor = m;
		}
	}
	
	public Piece (ChessBoard world, boolean isObserver, PieceAction[] actions, Piece mentor){
		this.mentor = mentor;
		this.world = world;
		this.isObserver = isObserver;
		this.actions = actions;
		this.tally = new int[this.world.numStates()][this.world.numStates()];
		this.actionTally = new int[this.world.numStates()][actions.length][this.world.numStates()];
		this.vMap = new double [this.world.numStates()];
		this.visibleQMap = new double[this.world.numStates()];
		//For Observers, the action set is augmented by a_m, the unknown
		if (isObserver){
			this.qMap = new double[this.world.numStates()][actions.length+1];
			this.MENTOR_ACTION_INDEX = actions.length;
			//Using this experiences of the  mentor to fill in the qFunction for the unknown mentor action;
			for (int s = 0; s < world.numStates(); s++){
				qMap[s][MENTOR_ACTION_INDEX] = mentor.visibleQMap[s];
			}
		} else {
			this.qMap = new double[this.world.numStates()][actions.length];
		}	
	}	
	
	//This method will give a observer the probability transition function
	//of the mentor
	public double transitionProb(int s, int t){
		int totalTransitionsFromS = 0;
		for (int i = 0; i < tally[s].length; i++){
			totalTransitionsFromS += tally[s][i];
		}
		if(totalTransitionsFromS == 0){
			//The Piece has no model, transitions to any t with 
			//equal probability
			return 1.0/actions.length;
		}
		return (double)(tally[s][t]/totalTransitionsFromS);
	}
	
	//This method will give an agents, its own probability transition function
	//The probability that the agent transitions to t when the agent is in state s and chooses action a
	private double transitionProb(int s, int a, int t){
		int total_S_A = 0;
		for (int j = 0; j < actionTally[s][a].length; j++){
			total_S_A += actionTally[s][a][j];
		}
		if(total_S_A == 0){
			//The Piece has no model, transitions to any t with 
			//equal probability
			return 1.0/actions.length;
		}
		return (double)(actionTally[s][a][t]/total_S_A);
	}
	

	public boolean isObserver() {
		return isObserver;
	}

	public int getNextAction() {
		double mentorExpectation = Double.NEGATIVE_INFINITY;
		double ownExpectation = Double.NEGATIVE_INFINITY;
		double bestExpectation;
		double bestActionValue;
		int bestAction;
		//EPSILON = EPSILON - 0.005;
		int s = world.getStateId(pos.getX(), pos.getY());
		if (world.QLEARNING){						
			//Epsilon Greedy Policy
			if (r.nextDouble() < EPSILON) {
				return r.nextInt(actions.length);
			} else {
				//Do the best action
				bestActionValue = Double.NEGATIVE_INFINITY;
				bestAction = 0;
				for (int i = 0; i < actions.length; i++) {
					if (qMap[s][i] > bestActionValue) {
						bestActionValue = qMap[s][i];
						bestAction = i;
					}
				}
				if (isObserver() && bestAction == MENTOR_ACTION_INDEX) {
					bestAction = kldMinimizer(s);
				}
			}
			
			return bestAction;
			
		} else {
			//Augmented Bellmann Backup
			//Epsilon Greedy Policy
			if (r.nextDouble() < EPSILON) {
				return r.nextInt(actions.length);
			} else {
				//First find the best action to take, given own model
				bestAction = 0;
				ownExpectation = Double.NEGATIVE_INFINITY;
				for (int a = 0; a < actions.length; a++){
					double sum = 0.0;
					for (int t = 0; t < world.numStates(); t++){
						sum += transitionProb(s, a, t)*vMap[t];
					}
					if (sum > ownExpectation){
						ownExpectation = sum;
						bestAction = a;
					}
				}
				bestExpectation = ownExpectation;
				if(isObserver){
					//Use the mentors information
					mentorExpectation = 0.0;
					for (int t = 0; t < world.numStates(); t++){
						mentorExpectation += mentor.transitionProb(s, t)*vMap[t];
					}							
					if (mentorExpectation > bestExpectation){
					//Find own action that minimizes the kL-Distance:
						bestAction = kldMinimizer(s);
						bestExpectation = mentorExpectation;
					}
				}
//				if (Math.abs(bestExpectation) < 0.01){
//					//Choose a random action
//					bestAction = r.nextInt(actions.length);
//				}

				updateVFunction(s, bestExpectation);
				return bestAction;			
			}
			
		}		
	}

	

	private int kldMinimizer(int s) {
		//Find the action that minimizes the KL-Distance
		double smallestKLDistance = Double.POSITIVE_INFINITY;
		int bestAction = r.nextInt(actions.length);
		double sum;
		for (int a = 0; a < actions.length; a++){
			sum = 0.0;
			for(int t = 0; t < world.numStates(); t++){
				sum += -1*transitionProb(s, a, t)*Math.log(mentor.transitionProb(s, t));
			}
			if (sum < smallestKLDistance){
				smallestKLDistance = sum;
				bestAction = a;
			}
		}
		return bestAction;
	}

	public void updateTally(int s, int a, int t) {
		tally[s][t]++;	
		actionTally[s][a][t]++;
	}
	
	public void updateVFunction(int s, double bestExpectation) {
		vMap[s] = (1.0 - ALPHA)*vMap[s] + ALPHA*world.getReward(s) + ALPHA*GAMMA*bestExpectation;		
	}

	public void updateQFunction(int s, int a, int t) {
		//Maximizing next action:
		double bestNextActionValue = Double.NEGATIVE_INFINITY;
		for (int a_Prime = 0; a_Prime < actions.length; a_Prime++){
			if (qMap[t][a_Prime] > bestNextActionValue){
				bestNextActionValue = qMap[t][a_Prime];
			}
		}		
		qMap[s][a]	= (1.0 - ALPHA)*qMap[s][a] + ALPHA*(world.getReward(t) + GAMMA*bestNextActionValue);	
		visibleQMap[s] = (1.0 - ALPHA)*visibleQMap[s] + ALPHA*(world.getReward(t) + GAMMA*bestNextActionValue);
	}

}
